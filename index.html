<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Humanoids 2024 - Real World Physical and Social Human-Robot Interaction</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
	    background-image: url("robothuman_boxcarry.jpeg");
	    background-size: cover;
	    background-position: center;
	    opacity: 0.95;
        }
        .navbar {
            margin-bottom: 20px;
        }
        .container {
            margin-top: 20px;
        }
        .footer {
            margin-top: 20px;
            padding: 20px 0;
            background-color: #f8f9fa;
            text-align: center;
        }
        .schedule {
            display: none;
        }
	
        /* Styles for all  jumbotron */
        .jumbotron {
            background-color: #ddffff; /* Lighter blue background for Program */
	}

        /* Styles for the "Home" jumbotron */
	.home-jumbotron {
            background-color: #ddffff; /* Light blue background for Home */
	}

	.speaker-img {
        width: 200px; /* Set a fixed width */
        height: 200px; /* Set a fixed height */
        object-fit: contain; /* Ensures the image is properly cropped */
        border-radius: 50%; /* Optional: Make the images circular */
        display: block;
        margin: 0 auto 10px; /* Center the image */
        }

        .speaker-card {
        text-align: center; /* Center text and image */
        margin-bottom: 30px;
       }

       .organizer-img {
        width: 150px; /* Set a fixed width */
        height: 150px; /* Set a fixed height */
        object-fit: contain; /* Ensures the image is properly cropped */
        border-radius: 50%; /* Optional: Make the images circular */
        display: block;
        margin: 0 auto 10px; /* Center the image */
        }

        .organizer-card {
        text-align: center; /* Center text and image */
        margin-bottom: 30px;
       }

	

    </style>
</head>
<body>

<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">RW-HRI Humanoids 2024</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link" href="#" onclick="showSection('home')">Home</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#" onclick="showSection('program')">Program</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#" onclick="showSection('speakers')">Speakers</a>
            </li>
	    <li class="nav-item">
                <a class="nav-link" href="#" onclick="showSection('accepted-papers')">Accepted Papers for Spotlight</a>
            </li>
	    <li class="nav-item">
                <a class="nav-link" href="#" onclick="showSection('contributions')">Call for Contributions</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#" onclick="showSection('contact')">Contact</a>
            </li>
        </ul>
    </div>
</nav>

<div class="container" id="home">
    <div class="jumbotron home-jumbotron">
        <h1 class="display-4">Workshop on Real World Physical and Social Human-Robot Interaction</h1>
        <p class="lead">Humanoids 2024</p>
        <hr class="my-4">
	<p> The workshop is held in hybrid mode, Please Join the Zoom Meeting: https://kth-se.zoom.us/j/67355684591 , Meeting ID: 673 5568 4591</p>
	<p> If you are attending, please fill up this form alongwith some questions for the panelists in our panel discussions here https://forms.gle/E1KXyWf526WibZtz9 </p>
	    
        <p>As robots increasingly enter everyday settings—from homes to workplaces—the necessity for sophisticated human-robot interaction (HRI) capabilities becomes paramount. Traditional HRI systems often rely on a single mode of interaction, which can limit the robot’s ability to understand and respond to human nuances effectively. Multimodal HRI seeks to overcome these limitations by integrating various sensory inputs such as visual, auditory, and tactile feedback, thus enabling robots to interpret and adapt to complex human behaviors and environments. However, the integration of these modalities presents significant challenges, including sensor fusion, context-aware computing, and the development of adaptive, user-centered interfaces that can handle diverse human expressions and intentions. </p>

 

<p>This workshop aims to convene leading scholars and practitioners to explore the integration of multiple modalities in robotic systems for enhanced human-robot interaction. This workshop will highlight recent advancements in tactile feedback, visual recognition, interaction patterns and social dynamics to create robots that can engage more naturally and effectively with humans in diverse environments. Building on the success of previous related workshops at renowned conferences such as ICRA and IROS, this session is anticipated to attract a broad audience, ranging from academic researchers and industrial practitioners to educators and policy makers. Participants will engage in a series of keynote presentations, interactive panels, demonstration, and hands-on demonstrations, providing both foundational insights and innovative approaches to multimodal interaction. The workshop will also feature a call for papers, inviting contributions that address theoretical models, empirical studies, or state-of-the-art applications in human-robot interaction. Through this comprehensive format, the workshop will foster an inclusive dialogue aimed at shaping the future directions of research and development in the field. </p>
    </div>
</div>

<div class="container schedule" id="program">
    <div class="jumbotron">
        <h2>Program Schedule</h2>
	<p> The workshop is held in hybrid mode, Please Join the Zoom Meeting: <a href="https://kth-se.zoom.us/j/67355684591" target="_blank">Click to join</a> , Meeting ID: 673 5568 4591</p>
	<p> If you are attending, please fill up this form alongwith some questions for the panelists in our panel discussions here: <a href="https://forms.gle/E1KXyWf526WibZtz9" target="_blank"> Click here for the FORM </a></p>

        <table class="table table-bordered">
            <thead>
                <tr>
                    <th>Time</th>
                    <th>Activity</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>09:00 - 09:10</td>
                    <td>Introduction</td>
                </tr>
                <tr>
                    <td>09:10 - 09:50</td>
                    <td>Invited Speaker: Dr. Katja Mombaur, “Physical-social interactions between humans and assistive robots in close proximity” (30 min + 10 min Q/A)</td>
                </tr>
                <tr>
                    <td>09:50 - 10:30</td>
                    <td>Invited Speaker: Dr. Eiichi Yoshida, "Human Model for Physical Human-Robot Interaction" (30 min + 10 min Q/A)</td>
                </tr>
                <tr>
                    <td>10:30 - 11:00</td>
                    <td>Coffee Break and Poster Presentation</td>
                </tr>
                <tr>
                    <td>11:00 - 11:30</td>
                    <td>Spotlight Talks: 
		    	<ul>
			    <li>"Failure Communication in Human-Robot Collaboration with Multimodal AI and Large Language Models." (<a href="https://drive.google.com/file/d/17FyULXfKctMMu3giRJEkl34N8pR2bT-C/view?usp=sharing" target="_blank">Link to the paper</a>)</li>
			    <li>"Event-Based Visual Servoing for Human-Robot Navigation using Reinforcement Learning." (<a href="https://drive.google.com/file/d/1sawVUgEn3a11qknZ2KRYUbnzB9J9Iagd/view?usp=sharing" target="_blank">Link to the paper</a>)</li>
			</ul>
		    </td>
                </tr>
                <tr>
                    <td>11:30 - 12:30</td>
                    <td>Panel Discussion: Why Should Physical and Social HRI Researchers Listen to Each Other More? (40 min + 20 min Q/A)</td>
                </tr>
                <tr>
                    <td>12:30 - 13:30</td>
                    <td>Lunch</td>
                </tr>
                <tr>
                    <td>13:30 - 14:10</td>
                    <td>Invited Speaker: Quentin Rouxel and Dionis Totsila, "LLMs, Diffusion and Humanoid Robots: Natural Language and Imitation Learning for Contact Interaction" (30 min + 10 min Q/A)</td>
                </tr>
                <tr>
                    <td>14:10 - 14:50</td>
                    <td>Invited Speaker: Enrico Mingo Hoffman, "OpenSoT: A Software Tool for Advanced Whole-Body Control." (30 min + 10 min Q/A)</td>
                </tr>
                <tr>
                    <td>14:50 - 15:30</td>
                    <td>Spotlight Talks: 
                        <ul>
                            <li>"Feasibility Study on a Multi-Device Dexterous Hand Teleoperation System for Daily Activity Performance in HRI" (<a href="https://drive.google.com/file/d/1I9Br5eDn9Db3h3Ep4AZnYvjFOG1Oi7t2/view?usp=sharing" target="_blank">Link to the paper</a>)</li>
                            <li>"Automated Gaze Labelling for Measuring Emotional and Cognitive Engagement in School-Age Children During Storytelling Activities with NAO Robot" (<a href="https://drive.google.com/file/d/1ry28GjzqZo0bA_hFdpDWqIKUDeorQXf_/view?usp=sharing" target="_blank">Link to the paper</a>)</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>15:30 - 16:00</td>
                    <td>Coffee Break and Poster Presentation</td>
                </tr>
                <tr>
                    <td>16:00 - 16:40</td>
                    <td>PAL Robotics (30 min + 10 min Q/A)</td>
                </tr>
		<tr>
                    <td>16:40 - 17:40</td>
                    <td>Panel Discussion: How to Reconcile Academia and Industry's Approach to HRI? (40 min + 20 min Q/A)</td>
                </tr>
                
                <tr>
                    <td>17:40 - 17:50</td>
                    <td>Concluding Remarks</td>
                </tr>
            </tbody>
        </table>
    </div>
</div>


<div class="container schedule" id="speakers">
<div class="jumbotron home-jumbotron">
        <h2>Speakers</h2>
        <p>Meet our esteemed speakers from academia and industry.</p>
    

    <div class="row">
        <!-- Speaker 1 -->
        <div class="col-md-6">
            <div class="speaker-card">
                <img src="katja.jpeg" alt="Katja Mombaur" class="speaker-img">
                <h3>Katja Mombaur</h3>
                <p>Professor, Karlsruhe Institute of Technology, Germany, and University of Waterloo, Canada</p>
                <p><strong>Presentation: Physical-social interactions between humans and assistive robots in close proximity</strong> TBD</p>
            </div>
        </div>

	<!-- Speaker 2 -->
        <div class="col-md-6">
            <div class="speaker-card">
                <img src="eiichi.jpeg" alt="Eiichi Yoshida" class="speaker-img">
                <h3>Eiichi Yoshida</h3>
                <p>Professor, Tokyo University of Science, Japan</p>
                <p><strong>Presentation:Human Model for Physical Human-Robot Interaction</strong> TBD</p>
            </div>
        </div>

        
    </div>

    <div class="row">
        
	<!-- Speaker 3 -->
        <div class="col-md-6">
            <div class="speaker-card">
                <img src="enrico.jpg" alt="Enrico Mingo Hoffman" class="speaker-img">
                <h3>Enrico Mingo Hoffman</h3>
                <p>ISFP Researcher, Centre Inria de l'Université de Lorraine & Loria, Nancy, France</p>
                <p><strong>Presentation:</strong> "OpenSoT: A Software Tool for Advanced Whole-Body Control"</p>
            </div>
        </div>
	<!-- Speaker 4 -->
        <div class="col-md-6">
            <div class="speaker-card">
                <img src="palrobotics.png" alt="Pal Robotics" class="speaker-img">
                <h3>Pal Robotics</h3>
                <p><strong>Presentation:</strong> TBD</p>
            </div>
        </div>
    </div>

    <div class="row">
        <!-- Speaker 5 -->

        <div class="col-md-6">
            <div class="speaker-card">
                <img src="toappear.jpg" alt="Quentin Rouxel" class="speaker-img">
                <h3>Quentin Rouxel</h3>
                <p>Postdoctoral Researcher, Inria Nancy - Grand Est, CNRS, Université de Lorraine, Villers-lés-Nancy, France</p>
                <p><strong>Presentation:</strong> "LLMs, Diffusion and Humanoid Robots: Natural Language and Imitation Learning for Contact Interaction"</p>
            </div>
        </div>

    </div>
</div>
</div>

<div class="container spotlight" id="accepted-papers">
    <div class="jumbotron">
        <h2>Accepted Papers for Spotlight Presentations</h2>
        <div class="paper">
            <h4>1. Event-Based Visual Servoing for Human-Robot Navigation using Reinforcement Learning</h4>
            <h5>Abstract:</h5>
            <p>Author: Ignacio Bugueno Cordova , Abstract—This work presents a visual servoing controller for social robots using event cameras and reinforcement learning, specifically designed to support safe, adaptive, and socially- aware human-robot interactions. The proposed controller en- ables real-time navigation and obstacle avoidance in dynamic environments by integrating event-based visual feedback and learning-based policy optimization. The results highlight the ap- proach’s robustness in managing physical and social interaction challenges, adapting smoothly to changes in human motion and environmental conditions. A demo video can be watched at: https://youtu.be/dF8 ektJ8Nk.</p>
            <p><a href="https://drive.google.com/file/d/1yRD_vVFH5CUL-VmBiTdeq0JhCHj82OeK/view?usp=sharing" target="_blank">Link to the paper</a></p>
        </div>
        <div class="paper">
            <h4>2. Feasibility Study on a Multi-Device Dexterous Hand Teleoperation System for Daily Activity Performance in HRI</h4>
            <h5>Abstract:</h5>
            <p>This study introduces a novel, non-invasive, multi- device hand-tracking system designed for dexterous teleoperation, with an emphasis on human grasp recognition for precise control of robotic manipulators. The proposed system leverages multiple visual sensors to improve accuracy and reduce occlusion-related tracking errors, allowing reliable detection of hand movements within an extended workspace. Implemented in a ROS-based framework, the system offers adaptability and scalability to additional robotic platforms. Twenty participants evaluated the system’s usability and reliability by performing two common daily activities using teleoperated grasp gestures. Results indicate high reliability, with a 94.17% success rate across trials, and positive user feedback, with 91.67% of users completing tasks successfully. Training effects were evident, with significant decreases in task completion time between early and late repetitions, reflecting enhanced user familiarity. Analysis of workload via NASA-TLX scores showed reduced mental and effort demands in successive tasks, underscoring the system’s user-friendliness and intuitiveness with experience. Future work will explore integrating additional hand parameters, comparing performance with wearable-based teleoperation systems, and expanding control to include robotic arm movement, emulating adaptive human grasp strategies. This work provides a foundation for effective human-robot collaboration in shared environments, advancing robotic capability in intention reading and cooperative task execution.</p> <!-- Leave this empty for now -->
            <p><a href="https://drive.google.com/file/d/1I9Br5eDn9Db3h3Ep4AZnYvjFOG1Oi7t2/view?usp=sharing" target="_blank">Link to the paper</a></p>
        </div>
        <div class="paper">
            <h4>3. Automated Gaze Labelling for Measuring Emotional and Cognitive Engagement in School-Age Children During Storytelling Activities with NAO Robot</h4>
            <h5>Abstract:</h5>
            <p>Modeling effective human-robot interaction requires the robot to detect and respond to engagement signals, which include attention, interest, and empathy. This study explores the application of automated gaze-labelling techniques to assess engagement in child-robot interactions using the NAO robot in a storytelling paradigm. A total of 72 children, aged 7 to 9, participated in structured individual sessions recorded for gaze analysis and engagement scoring. Engagement measures were manually annotated by observers using the Engagement Observation Scale and analyzed in correlation with automated gaze data processed using Gaze360 and K-means clustering for automated labelling. Results indicate high levels of engagement, with children directing their gaze toward the robot and interactive environment for most of the session time. The automatic gaze-labeling method showed a significant correlation with observer-assessed engagement scores, underscoring its potential as an efficient alternative to traditional methods.</p> <!-- Leave this empty for now -->
            <p><a href="https://drive.google.com/file/d/1ry28GjzqZo0bA_hFdpDWqIKUDeorQXf_/view?usp=sharing" target="_blank">Link to the paper</a></p>
        </div>
	<div class="paper">
            <h4>4. Failure Communication in Human-Robot Collaboration with Multimodal AI and Large Language Models</h4>
            <h5>Abstract:</h5>
            <p>This research presents a complete approach to failure communication in Human-Robot Collaboration (HRC) by integrating Multimodal AI and Large Language Models (LLMs). By combining human behavioral analysis with LLMs, the goal is to enhance the efficiency, fluidity, and naturalness of HRC interactions. The proposed framework enables robots to proactively predict and adapt the level of failure explanation based on observed human behavior, preempting potential con- fusion. LLMs generate responses tailored to a selected level of explanation, while follow-up interactions are designed to miti- gate confusion and enhance user comprehension and trust. The proposed framework aims to refine the collaborative experience, fostering more intuitive, adaptive, and efficient interactions between humans and robots across various application domains in real-world environments.</p>
            <p><a href="https://drive.google.com/file/d/17FyULXfKctMMu3giRJEkl34N8pR2bT-C/view?usp=sharing" target="_blank">Link to the paper</a></p>
        </div>
        
    </div>
</div>
	
<div class="container schedule" id="contributions">
    <div class="jumbotron">
        <h2>Call for Contributions</h2>
        <p>
            <strong>Date:</strong> November 22, 2024 (Full-day workshop)<br>
            <strong>Location:</strong> Hybrid (Nancy, France and online), as part of The 2024 IEEE-RAS International Conference on Humanoid Robots, IEEE-Humanoids 2024.<br>
            <strong>Submission Instruction:</strong> Email your contributions to: <a href="mailto:whsop.realworld.hri@gmail.com">whsop.realworld.hri@gmail.com</a><br>
            <strong>Contact for submissions:</strong> <a href="mailto:paragk@kth.se">paragk@kth.se</a>,<a href="mailto:e.yadollahi@lancaster.ac.uk">e.yadollahi@lancaster.ac.uk</a>
        </p>
        
        <h3>IMPORTANT DATES</h3>
        <ul>
            <li><strong>Submission deadline:</strong> November 12, 2024</li>
            <li><strong>Notification of acceptance:</strong> November 15, 2024</li>
            <li><strong>Camera-ready deadline:</strong> November 18, 2024</li>
            <li><strong>Workshop:</strong> November 22, 2024</li>
        </ul>
        <p>All deadlines are at 23:59 Anywhere on Earth time.</p>
        <hr>

        <h3>SUBMISSION GUIDELINES</h3>
        <p>
            Manuscripts should be written in English and will undergo a single-blind review by the organizing committee. The length should be 2-4 pages excluding references. We welcome contributions that include work in progress, preliminary results, technical reports, case studies, surveys, and state-of-the-art research. Position papers are also welcome and should be at least 2 pages excluding references. These can be research project proposals or plans without results. Authors must use the Humanoids templates provided, formatted for US Letter. The templates can be downloaded below.
        </p>
        <p><strong>Manuscript Templates:</strong> <a href="https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fras.papercept.net%2Fconferences%2Fsupport%2Ftex.php&data=05%7C02%7Cyadollah%40live.lancs.ac.uk%7C4ad413befc614cd18bf508dc7660b31a%7C9c9bcd11977a4e9ca9a0bc734090164a%7C0%7C0%7C638515404894323557%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=%2Fi%2B5zkzbpQa%2BM360ywxP0vBQR9gPfPs0vTolpHYrqS8%3D&reserved=0" target="_blank">LaTeX</a>, <a href="https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fras.papercept.net%2Fconferences%2Fsupport%2Fword.php&data=05%7C02%7Cyadollah%40live.lancs.ac.uk%7C4ad413befc614cd18bf508dc7660b31a%7C9c9bcd11977a4e9ca9a0bc734090164a%7C0%7C0%7C638515404894330332%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=XltFZwuZhQFRBmFIKSUVuFWXg27Yr8qTTtIofTl%2FbmI%3D&reserved=0" target="_blank">Word</a></p>
    </div>
</div>


<div class="container schedule" id="contact">
 <div class="jumbotron">
    <h2>Contact</h2>
    <p><strong>Organizers:</strong></p>
    <div class="row">
        <!-- Organizer 1 -->
        <div class="col-md-4">
            <div class="organizer-card">
                <img src="elmira.jpeg" alt="Elmira Yadollahi" class="organizer-img">
                <h3>Elmira Yadollahi</h3>
                <p>Assistant Professor, Lancaster University, United Kingdom <a href="mailto:e.yadollahi@lancaster.ac.uk">e.yadollahi@lancaster.ac.uk</a> </p>
            </div>
        </div>

	<!-- Organizer 2 -->
        <div class="col-md-4">
            <div class="organizer-card">
                <img src="parag.jpeg" alt="Parag Khanna" class="organizer-img">
                <h3>Parag Khanna</h3>
                <p>Doctoral Student, KTH Royal Institute of Technology, Sweden <a href="mailto:paragk@kth.se">paragk@kth.se</a></p>
            </div>
        </div>

	<!-- Organizer 3 -->
        <div class="col-md-4">
            <div class="organizer-card">
                <img src="ziwei.jpg" alt="Ziwei Wang" class="organizer-img">
                <h3>Ziwei Wang</h3>
                <p>Assistant Professor, Lancaster University, United Kingdom <a href="mailto:z.wang82@lancaster.ac.uk">z.wang82@lancaster.ac.uk</a></p>
            </div>
        </div>
    </div>
     <div class="row">
        <!-- Organizer 1 -->
        <div class="col-md-4">
            <div class="organizer-card">
                <img src="angela.jpg" alt="Elmira Yadollahi" class="organizer-img">
                <h3>Angela Faragasso</h3>
                <p> Lead Researcher Engineer, Finger Vision Inc., Japan<a href="mailto:angela.faragaso@fingervision.biz">angela.faragaso@fingervision.biz</a></p>
            </div>
        </div>

	<!-- Organizer 2 -->
        <div class="col-md-4">
            <div class="organizer-card">
                <img src="barbra.jpeg" alt="Barbara Bruno" class="organizer-img">
                <h3>Barbara Bruno</h3>
                <p>Junior Professor, Karlsruhe Institute of Technology, Germany: <a href="mailto:barbara.bruno@kit.edu">barbara.bruno@kit.edu</a></p>
            </div>
        </div>

	<!-- Organizer 3 -->
        <div class="col-md-4">
            <div class="organizer-card">
                <img src="chirstian.jpg" alt="Christian Smith" class="organizer-img">
                <h3>Christian Smith</h3>
                <p>Associate Professor, KTH Royal Institute of Technology, Sweden: <a href="mailto:ccs@kth.se">ccs@kth.se</a></p>
            </div>
        </div>
    </div>
 </div>
</div>
<div class="footer">
    <p>&copy; 2024 Humanoids RW-HRI. All rights reserved.</p>
</div>

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
<script>
    function showSection(sectionId) {
        // Hide all sections
        document.querySelectorAll('.container').forEach(function(section) {
            section.style.display = 'none';
        });
        // Show the selected section
        document.getElementById(sectionId).style.display = 'block';
    }
    
    // Show the home section by default
    showSection('home');
</script>
</body>
</html>
